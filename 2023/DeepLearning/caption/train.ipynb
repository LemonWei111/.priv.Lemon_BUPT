{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、任务说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文件代码展示ResNet101局部表示+LSTM+注意力机制模型训练过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、实验数据（参见getdata.py说明）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三、实验环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "硬件要求： CPU或支持GPU加速的硬件设备，如NVIDIA GPU，用于加速深度学习模型的训练和推理。\n",
    "\n",
    "操作系统：Windows10\n",
    "\n",
    "编写环境：jupyter notebook\n",
    "\n",
    "深度学习框架：依赖于深度学习框架PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "四、所用的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、使用ResNet101局部表示+LSTM+注意力机制模型训练实现。（模型详细说明参见models.py）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "相比vgg19整体表示+GRU模型改进：\n",
    "\n",
    "（1）图像编码器选择预训练的ResNet101，解码器选择LSTM。\n",
    "\n",
    "①ResNet101相较vgg19的优势在于：增加了输入到输出的直连，在输入和输出时都做了BN，能够缓解vgg19网络深带来的梯度消失难优化问题，提高训练速度的同时能够支持更高的学习率。\n",
    "\n",
    "②LSTM在数据相对充足的情况下表征能力较GRU更好。\n",
    "\n",
    "（2）引入微调，减小计算量，加快模型训练。\n",
    "\n",
    "①默认微调ResNet101的2-4个卷积块，相比以小学习率调整整个vgg19模型，计算量小，编码可靠性、准确性更高，能更快适应本例给出的新的编码场景。\n",
    "\n",
    "②默认微调词嵌入层。\n",
    "\n",
    "（3）使用参照dropout值的随机监督模式选择。\n",
    "\n",
    "每次训练产生随机值，当该随机值小于给定的dropout值，则使用有监督学习。\n",
    "\n",
    "（4）引入学习率衰减以更好地适应训练，防止模型过拟合“死掉”。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chubby\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'D:\\ProgramData\\Anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chubby\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chubby\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from models import Encoder, DecoderWithAttention\n",
    "from datasets import *\n",
    "from solver import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "from getdata import words,get_sequence,mktrainval\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、超参数说明：\n",
    "词嵌入的维度    'embed_dim' : 256\n",
    "注意力机制线性层的维度  'attention_dim' :256\n",
    "解码器 RNN 的维度   'decoder_dim' : 256\n",
    "丢弃率  'dropout' : 0.5\n",
    "批量大小    'batch_size' : 8\n",
    "编码器的学习率  'encoder_lr' : 1e-4\n",
    "解码器的学习率  'decoder_lr' : 4e-4\n",
    "梯度剪裁的绝对值阈值    'grad_clip' : 5.\n",
    "'doubly stochastic attention' 的正则化参数  'alpha_c' : 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这是一个配置字典，包含了训练过程中所需的各种参数。\n",
    "\n",
    "cfg = {\n",
    "    # Data parameters\n",
    "    'train_json_path': r'C:\\Users\\chubby\\Desktop\\训练测试划分\\train_captions.json',\n",
    "    'test_json_path': r'C:\\Users\\chubby\\Desktop\\训练测试划分\\test_captions.json',\n",
    "    'image_folder' : r'C:\\Users\\chubby\\Desktop\\code\\images', \n",
    "    'data_name' : 'DeepFashion_MultiModal',\n",
    "    # Model parameters\n",
    "    'embed_dim' : 256,  # dimension of word embeddings     # 词嵌入的维度\n",
    "    'attention_dim' :256,  # dimension of attention linear layers   # 注意力机制线性层的维度\n",
    "    'decoder_dim' : 256,  # dimension of decoder RNN  # 解码器 RNN 的维度\n",
    "    'dropout' : 0.5,  # 丢弃率\n",
    "    'device' : torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),  # sets device for model and PyTorch tensors \n",
    "    # 设置模型和 PyTorch 张量的设备\n",
    "    \n",
    "    # Training parameters\n",
    "    'start_epoch' : 0,# 开始的轮次\n",
    "    'epochs' : 10,  # number of epochs to train for (if early stopping is not triggered)# 训练轮次\n",
    "    'epochs_since_improvement' : 0,  # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
    "    # 记录自上次验证 BLEU 有所改善以来的轮次数\n",
    "    'batch_size' : 8,# 批量大小\n",
    "    'workers' : 1,  # for data-loading; right now, only 1 works  # 用于数据加载的工作线程数（当前只有 1 有效）\n",
    "    'encoder_lr' : 1e-4,  # learning rate for encoder if fine-tuning  # 如果进行微调，编码器的学习率\n",
    "    'decoder_lr' : 4e-4,  # learning rate for decoder  # 解码器的学习率\n",
    "    'grad_clip' : 5.,  # clip gradients at an absolute value of # 梯度剪裁的绝对值阈值\n",
    "    'alpha_c' : 1.,  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
    "    # 'doubly stochastic attention' 的正则化参数\n",
    "    'best_bleu4' : 0.,  # BLEU-4 score right now # 当前的 BLEU-4 分数\n",
    "    'print_freq' : 50,  # print training/validation stats every __ batches  # 每 __ 个批次打印训练/验证统计信息\n",
    "    'fine_tune_encoder' : False,  # fine-tune encoder or not  # 是否微调编码器\n",
    "    'checkpoint' : None,  # path to checkpoint, None if none # 检查点路径，如果没有则为 None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_map = words(cfg['train_json_path'])# 从训练集的 JSON 文件中获取单词映射\n",
    "cfg['vocab_size'] = len(word_map)# 将词汇表大小添加到配置中\n",
    "batch_size = cfg['batch_size']\n",
    "# 获取批量大小和数据加载器\n",
    "train_loader, val_loader = mktrainval(cfg['train_json_path'], \n",
    "                                                 cfg['test_json_path'], \n",
    "                                                 word_map, batch_size, \n",
    "                                                 workers=cfg['workers'],\n",
    "                                                 file_path=cfg['image_folder'])\n",
    "# 以下是对函数 `mktrainval` 的一些假设的注释\n",
    "# 该函数的目的是创建训练和验证数据加载器，返回两个 DataLoader 对象\n",
    "\n",
    "# for image, caption, caplen in train_loader:\n",
    "#     print(image, caption, caplen)\n",
    "#     break\n",
    "# 上述代码是为了展示一批次数据的示例，可以用于检查数据加载器是否正常工作\n",
    "# 实际训练时，会在训练循环中使用这些数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg['checkpoint'] is None:\n",
    "     # 如果没有提供检查点文件，创建新的编码器和解码器\n",
    "    encoder = Encoder()\n",
    "    encoder.fine_tune(cfg['fine_tune_encoder'])\n",
    "    encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()), # 是否对编码器进行微调\n",
    "    encoder_optimizer = (\n",
    "                                         lr=cfg['encoder_lr']) if cfg['fine_tune_encoder'] else None\n",
    "    decoder = DecoderWithAttention(cfg)\n",
    "    decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
    "                                         lr=cfg['decoder_lr'])\n",
    "else:\n",
    "                                         \n",
    "     # 如果提供了检查点文件，加载之前训练的模型和优化器状态\n",
    "    checkpoint = torch.load(cfg['checkpoint'])\n",
    "    cfg['start_epoch'] = checkpoint['epoch'] + 1\n",
    "    cfg['epochs_since_improvement'] = checkpoint['epochs_since_improvement']\n",
    "    cfg['best_bleu4'] = checkpoint['bleu-4']\n",
    "    encoder = checkpoint['encoder']\n",
    "    encoder_optimizer = checkpoint['encoder_optimizer']\n",
    "    decoder = checkpoint['decoder']\n",
    "    decoder_optimizer = checkpoint['decoder_optimizer']\n",
    "                                         \n",
    "    # 如果需要对编码器进行微调而且优化器为空，则创建一个新的编码器优化器\n",
    "    if cfg['fine_tune_encoder'] is True and encoder_optimizer is None:\n",
    "        encoder.fine_tune(cfg['fine_tune_encoder'])\n",
    "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                             lr=cfg['encoder_lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to GPU, if available\n",
    "decoder = decoder.to(cfg['device'])\n",
    "encoder = encoder.to(cfg['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3、损失函数说明：\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(cfg['device'])\n",
    "\n",
    "这行代码创建了一个交叉熵损失函数的实例，并将其移动到配置中指定的设备（GPU 或 CPU）。\n",
    "\n",
    "然后使用交叉熵损失函数计算模型生成的概率分布与实际标注的概率分布之间的差异。\n",
    "\n",
    "nn.CrossEntropyLoss 通常用于多类别分类任务，其输入是模型的输出分数和实际目标类别的索引。在图像标注任务中，这个损失函数常用于计算模型生成的单词序列和实际标注之间的差异。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss().to(cfg['device'])\n",
    "#这行代码创建了一个交叉熵损失函数的实例，并将其移动到配置中指定的设备（GPU 或 CPU）。\n",
    "# 然后使用交叉熵损失函数计算模型生成的概率分布与实际标注的概率分布之间的差异。\n",
    "# nn.CrossEntropyLoss 通常用于多类别分类任务，其输入是模型的输出分数和实际目标类别的索引。在图像标注任务中，这个损失函数常用于计算模型生成的单词序列和实际标注之间的差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "五、实验结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/1270]\tBatch Time 20.083 (20.083)\tData Load Time 17.366 (17.366)\tLoss 5.7583 (5.7583)\tTop-5 Accuracy 2.114 (2.114)\n",
      "Epoch: [0][50/1270]\tBatch Time 0.387 (0.756)\tData Load Time 0.000 (0.341)\tLoss 4.7605 (5.0050)\tTop-5 Accuracy 35.729 (29.523)\n",
      "Epoch: [0][100/1270]\tBatch Time 0.373 (0.562)\tData Load Time 0.000 (0.172)\tLoss 4.5885 (4.8537)\tTop-5 Accuracy 39.565 (33.278)\n",
      "Epoch: [0][150/1270]\tBatch Time 0.369 (0.499)\tData Load Time 0.000 (0.115)\tLoss 4.0547 (4.6818)\tTop-5 Accuracy 55.381 (37.741)\n",
      "Epoch: [0][200/1270]\tBatch Time 0.388 (0.469)\tData Load Time 0.001 (0.087)\tLoss 3.7992 (4.4576)\tTop-5 Accuracy 63.441 (43.863)\n",
      "Epoch: [0][250/1270]\tBatch Time 0.373 (0.451)\tData Load Time 0.000 (0.069)\tLoss 3.1870 (4.2209)\tTop-5 Accuracy 76.549 (49.845)\n",
      "Epoch: [0][300/1270]\tBatch Time 0.393 (0.439)\tData Load Time 0.000 (0.058)\tLoss 3.4604 (4.0202)\tTop-5 Accuracy 69.679 (54.716)\n",
      "Epoch: [0][350/1270]\tBatch Time 0.374 (0.430)\tData Load Time 0.000 (0.050)\tLoss 2.6619 (3.8507)\tTop-5 Accuracy 87.013 (58.676)\n",
      "Epoch: [0][400/1270]\tBatch Time 0.387 (0.424)\tData Load Time 0.000 (0.044)\tLoss 2.5828 (3.7089)\tTop-5 Accuracy 87.708 (61.917)\n",
      "Epoch: [0][450/1270]\tBatch Time 0.389 (0.419)\tData Load Time 0.000 (0.039)\tLoss 2.6062 (3.5881)\tTop-5 Accuracy 85.595 (64.621)\n",
      "Epoch: [0][500/1270]\tBatch Time 0.408 (0.415)\tData Load Time 0.001 (0.035)\tLoss 2.4906 (3.4810)\tTop-5 Accuracy 88.934 (66.986)\n",
      "Epoch: [0][550/1270]\tBatch Time 0.375 (0.412)\tData Load Time 0.000 (0.032)\tLoss 2.3170 (3.3821)\tTop-5 Accuracy 91.686 (69.112)\n",
      "Epoch: [0][600/1270]\tBatch Time 0.374 (0.409)\tData Load Time 0.000 (0.029)\tLoss 2.4529 (3.3006)\tTop-5 Accuracy 90.300 (70.855)\n",
      "Epoch: [0][650/1270]\tBatch Time 0.400 (0.407)\tData Load Time 0.000 (0.027)\tLoss 2.2652 (3.2270)\tTop-5 Accuracy 92.990 (72.404)\n",
      "Epoch: [0][700/1270]\tBatch Time 0.382 (0.405)\tData Load Time 0.000 (0.025)\tLoss 2.3699 (3.1632)\tTop-5 Accuracy 90.517 (73.731)\n",
      "Epoch: [0][750/1270]\tBatch Time 0.357 (0.404)\tData Load Time 0.001 (0.023)\tLoss 2.1160 (3.1052)\tTop-5 Accuracy 93.968 (74.922)\n",
      "Epoch: [0][800/1270]\tBatch Time 0.388 (0.404)\tData Load Time 0.000 (0.022)\tLoss 2.1910 (3.0494)\tTop-5 Accuracy 94.053 (76.053)\n",
      "Epoch: [0][850/1270]\tBatch Time 0.438 (0.404)\tData Load Time 0.001 (0.021)\tLoss 2.2189 (2.9992)\tTop-5 Accuracy 93.377 (77.053)\n",
      "Epoch: [0][900/1270]\tBatch Time 0.394 (0.405)\tData Load Time 0.000 (0.019)\tLoss 2.2257 (2.9552)\tTop-5 Accuracy 91.741 (77.933)\n",
      "Epoch: [0][950/1270]\tBatch Time 0.429 (0.406)\tData Load Time 0.000 (0.018)\tLoss 2.4989 (2.9137)\tTop-5 Accuracy 89.737 (78.748)\n",
      "Epoch: [0][1000/1270]\tBatch Time 0.426 (0.408)\tData Load Time 0.000 (0.018)\tLoss 2.2017 (2.8778)\tTop-5 Accuracy 94.200 (79.454)\n",
      "Epoch: [0][1050/1270]\tBatch Time 0.432 (0.409)\tData Load Time 0.001 (0.017)\tLoss 2.1451 (2.8425)\tTop-5 Accuracy 93.182 (80.135)\n",
      "Epoch: [0][1100/1270]\tBatch Time 0.418 (0.410)\tData Load Time 0.000 (0.016)\tLoss 2.0853 (2.8101)\tTop-5 Accuracy 93.822 (80.774)\n",
      "Epoch: [0][1150/1270]\tBatch Time 0.427 (0.412)\tData Load Time 0.000 (0.015)\tLoss 2.0110 (2.7789)\tTop-5 Accuracy 95.475 (81.365)\n",
      "Epoch: [0][1200/1270]\tBatch Time 0.470 (0.413)\tData Load Time 0.000 (0.015)\tLoss 1.9808 (2.7512)\tTop-5 Accuracy 95.020 (81.881)\n",
      "Epoch: [0][1250/1270]\tBatch Time 0.451 (0.415)\tData Load Time 0.000 (0.014)\tLoss 1.9043 (2.7234)\tTop-5 Accuracy 96.312 (82.390)\n",
      "Validation: [0/318]\tBatch Time 4.976 (4.976)\tLoss 2.1751 (2.1751)\tTop-5 Accuracy 93.204 (93.204)\t\n",
      "Validation: [50/318]\tBatch Time 0.277 (0.360)\tLoss 1.9481 (1.9617)\tTop-5 Accuracy 96.017 (95.422)\t\n",
      "Validation: [100/318]\tBatch Time 0.268 (0.316)\tLoss 2.1749 (1.9492)\tTop-5 Accuracy 93.900 (95.618)\t\n",
      "Validation: [150/318]\tBatch Time 0.283 (0.301)\tLoss 1.8427 (1.9292)\tTop-5 Accuracy 96.349 (95.784)\t\n",
      "Validation: [200/318]\tBatch Time 0.336 (0.295)\tLoss 1.8624 (1.9348)\tTop-5 Accuracy 96.398 (95.742)\t\n",
      "Validation: [250/318]\tBatch Time 0.274 (0.291)\tLoss 1.7991 (1.9356)\tTop-5 Accuracy 98.121 (95.761)\t\n",
      "Validation: [300/318]\tBatch Time 0.279 (0.288)\tLoss 1.6178 (1.9300)\tTop-5 Accuracy 97.764 (95.827)\t\n",
      "Epoch: [1][0/1270]\tBatch Time 15.967 (15.967)\tData Load Time 15.312 (15.312)\tLoss 2.0461 (2.0461)\tTop-5 Accuracy 94.094 (94.094)\n",
      "Epoch: [1][50/1270]\tBatch Time 0.432 (0.696)\tData Load Time 0.000 (0.300)\tLoss 1.8396 (2.0474)\tTop-5 Accuracy 97.149 (94.675)\n",
      "Epoch: [1][100/1270]\tBatch Time 0.444 (0.570)\tData Load Time 0.000 (0.152)\tLoss 1.8240 (2.0551)\tTop-5 Accuracy 97.689 (94.532)\n",
      "Epoch: [1][150/1270]\tBatch Time 0.411 (0.528)\tData Load Time 0.000 (0.102)\tLoss 1.9888 (2.0548)\tTop-5 Accuracy 95.714 (94.556)\n",
      "Epoch: [1][200/1270]\tBatch Time 0.547 (0.508)\tData Load Time 0.000 (0.076)\tLoss 1.8203 (2.0500)\tTop-5 Accuracy 98.312 (94.556)\n",
      "Epoch: [1][250/1270]\tBatch Time 0.467 (0.495)\tData Load Time 0.000 (0.061)\tLoss 1.9392 (2.0520)\tTop-5 Accuracy 96.325 (94.564)\n",
      "Epoch: [1][300/1270]\tBatch Time 0.450 (0.487)\tData Load Time 0.000 (0.051)\tLoss 2.0617 (2.0508)\tTop-5 Accuracy 95.329 (94.592)\n",
      "Epoch: [1][350/1270]\tBatch Time 0.432 (0.480)\tData Load Time 0.001 (0.044)\tLoss 2.0016 (2.0497)\tTop-5 Accuracy 93.665 (94.607)\n",
      "Epoch: [1][400/1270]\tBatch Time 0.477 (0.476)\tData Load Time 0.001 (0.038)\tLoss 2.0499 (2.0404)\tTop-5 Accuracy 95.849 (94.728)\n"
     ]
    }
   ],
   "source": [
    "# Epochs\n",
    "for epoch in range(cfg['start_epoch'], cfg['epochs']):\n",
    "    \n",
    "     # 如果连续8个epoch没有改进，学习率衰减，并在20个epoch没有改进时终止训练\n",
    "    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
    "    if cfg['epochs_since_improvement'] == 20:\n",
    "        break\n",
    "    if cfg['epochs_since_improvement'] > 0 and cfg['epochs_since_improvement'] % 8 == 0:\n",
    "        adjust_learning_rate(decoder_optimizer, 0.8)\n",
    "        if cfg['fine_tune_encoder']:\n",
    "            adjust_learning_rate(encoder_optimizer, 0.8)\n",
    "\n",
    "      # 训练一个epoch\n",
    "    # One epoch's training\n",
    "    train(train_loader=train_loader,\n",
    "          encoder=encoder,\n",
    "          decoder=decoder,\n",
    "          criterion=criterion,\n",
    "          encoder_optimizer=encoder_optimizer,\n",
    "          decoder_optimizer=decoder_optimizer,\n",
    "          epoch=epoch,\n",
    "          cfg=cfg)\n",
    "    \n",
    "    # 验证一个epoch\n",
    "    # One epoch's validation\n",
    "    validate(val_loader=val_loader,\n",
    "                            encoder=encoder,\n",
    "                            decoder=decoder,\n",
    "                            criterion=criterion,\n",
    "                            word_map=word_map,\n",
    "                            cfg=cfg)\n",
    "\n",
    "      # 保存检查点\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(cfg['data_name'], epoch, cfg['epochs_since_improvement'], encoder, decoder, encoder_optimizer,\n",
    "                    decoder_optimizer, 0, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "六、实验结果分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 0:\n",
    "Loss: 初始损失为5.7583，并随着训练的进行逐渐减小，最终下降到2.7512。这表明模型在训练数据上逐渐学到了任务的模式。\n",
    "Top-5 Accuracy: 初始 Top-5 Accuracy 为2.114%，随着训练逐渐提高，最终达到了81.881%。这说明模型在前五个预测中包含正确标签的概率在训练过程中不断提高。\n",
    "Epoch 1:\n",
    "Loss: 新的 epoch 开始，损失重新升高（2.7512），然后逐渐减小到2.0404。这是因为模型继续学习并逐渐逼近更优解。\n",
    "Top-5 Accuracy: 随着训练的进行，Top-5 Accuracy 从81.881% 提高到94.728%。这表示模型在训练数据上的性能不断提升。\n",
    "\n",
    "Loss 变化： 损失逐渐减小表明模型在学习任务的过程中在一定程度上取得了成功。然而，如果在后续 epoch 中损失再次升高，可能需要考虑调整学习率或其他训练参数。\n",
    "\n",
    "Top-5 Accuracy 变化： Top-5 Accuracy 在训练过程中不断提高，这是一个良好的迹象。这意味着模型能够在前五个预测中更准确地包含正确标签。如果这一趋势持续，可以考虑增加训练周期以进一步提高性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "七、总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1、实现功能：\n",
    "\n",
    "（1）encoder： 通过CNN局部表示resnet101倒数特征层编码图像\n",
    "\n",
    "（2）decoder：通过RNN自注意力机制模型解码生成图像的描述性文本\n",
    "\n",
    "（3）Beam Search ： 采用了 Beam Search 解码方法来生成图像描述，这有助于获得更准确和多样的描述\n",
    "\n",
    "2、创新点：\n",
    "\n",
    "（1）Attention 机制： 代码中使用了 Attention 机制，能够使模型在生成描述时更加关注图像的不同部分。\n",
    "\n",
    "（2）相较vgg19整体表示+GRU模型的改善：在反思上一个模型的基础上，尝试新的编码器、解码器选择，针对编码器的预训练模型和词嵌入层，引入微调的策略，同时在训练过程中引入学习率衰减和基于dropout的随机监督模式选择，提升了模型的表征能力。\n",
    "\n",
    "3、不足与反思：\n",
    "\n",
    "（1）Overfitting 风险： 需要关注模型是否存在过拟合的风险，特别是在训练集上的表现很好，但在验证集上性能较差时。\n",
    "\n",
    "（2）训练参数调优： 日志中没有提到是否进行了超参数的调优。调整学习率、批量大小等参数可能会对模型性能产生重要影响。\n",
    "\n",
    "（3）模型解释性： 如果模型是黑盒的，难以解释模型如何生成描述。有时候，理解模型的决策是非常重要的。\n",
    "\n",
    "4、反思与改进方向：\n",
    "\n",
    "（1）监控和可视化： 在训练过程中添加更多的监控和可视化，例如绘制损失曲线、生成的描述示例等，有助于更好地了解模型的训练过程。\n",
    "\n",
    "（2）超参数搜索： 尝试不同的超参数组合，使用交叉验证等方法来找到最佳的模型配置。\n",
    "\n",
    "（3）模型评估： 使用更多的评价指标来全面评估模型性能，如BLEU、METEOR等，以更全面地了解模型生成描述的质量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
