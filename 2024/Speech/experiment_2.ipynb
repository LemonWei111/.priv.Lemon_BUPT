{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**实验目标：**\n",
    "\n",
    "通过本实验，你将深入了解和实践说话人识别技术，并掌握利用声音特征进行有效说话人识别的基本方法，了解不同特征和模型对识别准确率的影响。\n",
    "\n",
    "实验的核心目标是使用TIMIT数据集来训练一个说话人识别系统，涵盖数据预处理、特征提取、模型训练和评估等关键步骤。\n",
    "\n",
    "\n",
    "**实验方法：**\n",
    "\n",
    "**1. 数据预处理和划分(可选)：**\n",
    "  - 为了方便大家，我们提供了划分好的TIMIT数据集结构，当然你也可以根据训练结果自行划分该原数据集。\n",
    "  - 原数据集下载地址：https://drive.google.com/file/d/180mSIiXN9RVDV2Xn1xcWNkMRm5J5MjN4/view?usp=sharing\n",
    "  - 我们排除了SA的两个方言句子，并在剩余的8个句子中选取了SX的5个句子和SI的1个句子作为训练集，SI的另外2个句子作为测试集。\n",
    "  \n",
    "**2. 特征提取：**\n",
    "  - 学习并实现包括但不限于MFCC特征等特征的提取，探索声音信号的频率和时间特性。\n",
    "  - 鼓励尝试和比较其他特征提取方法，例如LPCC或声谱图特征，以理解不同特征对识别性能的影响。\n",
    "  \n",
    "**3. 模型选择和训练：**\n",
    "  - 探索并选择适合的分类器和模型进行说话人识别，如GMM、Softmax分类器或深度学习模型。\n",
    "  - 实现模型训练流程，使用训练集数据训练模型。\n",
    "  \n",
    "**4. 评估和分析：**\n",
    "  - 使用准确率作为主要的评价指标在测试集上评估模型性能。\n",
    "  - 对比不同特征和模型的性能，分析其对说话人识别准确率的影响。\n",
    "  - 可视化不同模型的识别结果和错误率，讨论可能的改进方法。\n",
    "\n",
    "**实验要求：**\n",
    "  - 1.选择并实现至少一种特征的提取，并鼓励尝试其他特征提取方法。\n",
    "  - 2.选择并实现至少一种分类器或模型进行说话人识别，并使用准确率评估指标评估其性能。\n",
    "  - 3.通过实验对比、分析和可视化，撰写详细的实验报告，包括实验目的、实验方法、结果分析和结论。\n",
    "  - 4.实验报告应以清晰、逻辑性强的形式呈现，图表和结果应清楚明了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实验准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 导入必要的库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 可以根据需要导入其他库，比如librosa用于音频处理\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# !pip install hmmlearn\n",
    "from hmmlearn import hmm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# !pip install --user tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理(加载数据集)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "[ 16  16  16 ... 422 422 422]\n",
      "462\n",
      "训练集大小: 3234\n",
      "测试集大小: 924\n"
     ]
    }
   ],
   "source": [
    "TrainDir = \"Dataset\\TRAIN\"\n",
    "TestDir = \"Dataset\\TEST\"\n",
    "## 请在这里写代码加载我们划分好的TIMIT训练集和测试集\n",
    "\n",
    "# 定义函数加载数据集\n",
    "def load_dataset(data_dir, le = None):\n",
    "    data = []\n",
    "    labels = []\n",
    "    print(os.path.exists(data_dir))\n",
    "    for speaker_dir in os.listdir(data_dir):\n",
    "        speaker_path = os.path.join(data_dir, speaker_dir)\n",
    "        for speaker_id in os.listdir(speaker_path):\n",
    "            speaker_id_path = os.path.join(speaker_path, speaker_id)\n",
    "            for file_name in os.listdir(speaker_id_path):\n",
    "                file_path = os.path.join(speaker_id_path, file_name)\n",
    "                # 在这里你可以处理每个音频文件，例如提取特征并将特征作为数据，将说话者ID作为标签\n",
    "                # 这里暂时只返回文件路径，具体特征提取需要在后续完成\n",
    "                data.append(file_path)\n",
    "                labels.append(speaker_id)\n",
    "    if le is None:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(labels)\n",
    "    y = le.transform(labels) # back: inverse_transform\n",
    "\n",
    "    return data, y, le\n",
    "\n",
    "# 加载训练集和测试集数据\n",
    "train_data, train_labels, le = load_dataset(TrainDir)\n",
    "test_data, test_labels, _ = load_dataset(TestDir, le)\n",
    "print(train_labels)\n",
    "print(len(set(train_labels)))\n",
    "for l in train_labels:\n",
    "    if l == len(set(train_labels)):\n",
    "        print(l)\n",
    "# 打印训练集和测试集大小\n",
    "print(\"训练集大小:\", len(train_data))\n",
    "print(\"测试集大小:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC特征形状: (20, 100, 5)\n",
      "(20, 100, 5) (20, 100, 5)\n"
     ]
    }
   ],
   "source": [
    "## 请编写或使用库函数提取MFCC等音频特征\n",
    "\n",
    "num_mfcc=20\n",
    "max_length = 100\n",
    "input_channels = 5\n",
    "# 定义函数提取MFCC特征\n",
    "def extract_mfcc(file_path, num_mfcc=20, max_length = 100, input_channels = 5): # (default num_mfcc: 20)\n",
    "    # 加载音频文件\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    # 提取MFCC特征\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=num_mfcc)\n",
    "    # mfccs = np.mean(mfccs.T, axis=0)  # 对时间轴上的MFCC进行平均\n",
    "\n",
    "    features = []\n",
    "    # 遍历每个MFCC特征序列\n",
    "    for index, feature_sequence in enumerate(mfccs):\n",
    "        processed_features = []\n",
    "        # 如果特征序列的长度大于最大长度，则切片成多段特征\n",
    "        if len(feature_sequence) > max_length:\n",
    "            num_slices = len(feature_sequence) // max_length\n",
    "            remainder = len(feature_sequence) % max_length\n",
    "            if remainder != 0:\n",
    "                num_slices += 1\n",
    "            for i in range(num_slices):\n",
    "                start_idx = i * max_length\n",
    "                end_idx = min((i + 1) * max_length, len(feature_sequence))\n",
    "                sliced_feature = feature_sequence[start_idx:end_idx]\n",
    "                # 如果切片长度不足最大长度，则用零填充\n",
    "                if len(sliced_feature) < max_length:\n",
    "                    padded_feature = np.zeros(max_length - len(sliced_feature))\n",
    "                    sliced_feature = np.concatenate((sliced_feature, padded_feature), axis=0)\n",
    "                processed_features.append(sliced_feature)\n",
    "        # 如果特征序列的长度小于最大长度，则用零填充\n",
    "        elif len(feature_sequence) < max_length:\n",
    "            padded_feature = np.zeros(max_length - len(feature_sequence))\n",
    "            padded_sequence = np.concatenate((feature_sequence, padded_feature), axis=0)\n",
    "            processed_features.append(padded_sequence)\n",
    "        # 如果特征序列的长度等于最大长度，则直接添加\n",
    "        else:\n",
    "            processed_features.append(feature_sequence)\n",
    "        processed_features = np.array(processed_features)\n",
    "        features.append(processed_features.T)\n",
    "    # 将列表转换为数组\n",
    "    features = np.array(features)\n",
    "    \n",
    "    # 初始化一个数组来存储处理后的特征\n",
    "    processed_features = np.empty((0, max_length, input_channels))\n",
    "\n",
    "    # 遍历每个MFCC特征序列\n",
    "    for feature_sequence in features:\n",
    "        # 进行通道变换\n",
    "        if feature_sequence.shape[1] != input_channels:\n",
    "            if feature_sequence.shape[1] < input_channels:\n",
    "                # 如果MFCC特征通道数少于期望的通道数，则在末尾填充零\n",
    "                padded_feature = np.zeros((feature_sequence.shape[0], input_channels - feature_sequence.shape[1]))\n",
    "                feature_sequence = np.concatenate((feature_sequence, padded_feature), axis=1)\n",
    "            else:\n",
    "                # 如果MFCC特征通道数多于期望的通道数，则保留前input_channels个通道\n",
    "                feature_sequence = feature_sequence[:, :input_channels]\n",
    "        # 将特征序列添加到数组中\n",
    "        processed_features = np.append(processed_features, [feature_sequence], axis=0)\n",
    "    \n",
    "    return processed_features # mfccs\n",
    "\n",
    "# 测试MFCC特征提取\n",
    "sample_mfcc = extract_mfcc(train_data[0])\n",
    "print(\"MFCC特征形状:\", sample_mfcc.shape)\n",
    "\n",
    "# 加载训练集和测试集数据\n",
    "train_mfcc = [extract_mfcc(file_path) for file_path in train_data]\n",
    "test_mfcc = [extract_mfcc(file_path) for file_path in test_data]\n",
    "\n",
    "print(train_mfcc[0].shape, train_mfcc[30].shape)\n",
    "# LPCC\n",
    "\n",
    "# 声谱图特征\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型选择和训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 在这部分，你可以选择不同的分类器和模型如GMM模型来进行实验\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'numpy.ndarray'> tensor([ 16,  16,  16,  ..., 422, 422, 422], dtype=torch.int32)\n",
      "2587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weijing\\AppData\\Local\\Temp\\ipykernel_17696\\2909764473.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [20/80], Loss: 6.1370\n",
      "Epoch [1/10], Batch [40/80], Loss: 6.1363\n",
      "Epoch [1/10], Batch [60/80], Loss: 6.1356\n",
      "Epoch [1/10], Batch [80/80], Loss: 6.1356\n",
      "Epoch [2/10], Batch [20/80], Loss: 6.1354\n",
      "Epoch [2/10], Batch [40/80], Loss: 6.1353\n",
      "Epoch [2/10], Batch [60/80], Loss: 6.1361\n",
      "Epoch [2/10], Batch [80/80], Loss: 6.1341\n",
      "Epoch [3/10], Batch [20/80], Loss: 6.1341\n",
      "Epoch [3/10], Batch [40/80], Loss: 6.1366\n",
      "Epoch [3/10], Batch [60/80], Loss: 6.1391\n",
      "Epoch [3/10], Batch [80/80], Loss: 6.1392\n",
      "Epoch [4/10], Batch [20/80], Loss: 6.1382\n",
      "Epoch [4/10], Batch [40/80], Loss: 6.1391\n",
      "Epoch [4/10], Batch [60/80], Loss: 6.1391\n",
      "Epoch [4/10], Batch [80/80], Loss: 6.1393\n",
      "Epoch [5/10], Batch [20/80], Loss: 6.1392\n",
      "Epoch [5/10], Batch [40/80], Loss: 6.1391\n",
      "Epoch [5/10], Batch [60/80], Loss: 6.1391\n",
      "Epoch [5/10], Batch [80/80], Loss: 6.1392\n",
      "Epoch [6/10], Batch [20/80], Loss: 6.1380\n",
      "Epoch [6/10], Batch [40/80], Loss: 6.1387\n",
      "Epoch [6/10], Batch [60/80], Loss: 6.1389\n",
      "Epoch [6/10], Batch [80/80], Loss: 6.1391\n",
      "Epoch [7/10], Batch [20/80], Loss: 6.1293\n",
      "Epoch [7/10], Batch [40/80], Loss: 6.1387\n",
      "Epoch [7/10], Batch [60/80], Loss: 6.1392\n",
      "Epoch [7/10], Batch [80/80], Loss: 6.1392\n",
      "Epoch [8/10], Batch [20/80], Loss: 6.1080\n",
      "Epoch [8/10], Batch [40/80], Loss: 6.1300\n",
      "Epoch [8/10], Batch [60/80], Loss: 6.1390\n",
      "Epoch [8/10], Batch [80/80], Loss: 6.1392\n",
      "Epoch [9/10], Batch [20/80], Loss: 6.1075\n",
      "Epoch [9/10], Batch [40/80], Loss: 6.1080\n",
      "Epoch [9/10], Batch [60/80], Loss: 6.1392\n",
      "Epoch [9/10], Batch [80/80], Loss: 6.1392\n",
      "Epoch [10/10], Batch [20/80], Loss: 6.1049\n",
      "Epoch [10/10], Batch [40/80], Loss: 6.1100\n",
      "Epoch [10/10], Batch [60/80], Loss: 6.1391\n",
      "Epoch [10/10], Batch [80/80], Loss: 6.1392\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# 定义CNN模型\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(3*5*25, 128)  # 根据输入尺寸计算\n",
    "        self.fc2 = nn.Linear(128, num_classes)  # 有num_classes个类别\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        # print(x.shape) # [32, 3, 10, 50]\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        # print(x.shape) # [32, 3, 5, 25]\n",
    "        x = x.reshape(x.shape[0], -1)  # 展平多维卷积层输出\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x)\n",
    "# 实例化模型\n",
    "num_classes = len(set(train_labels))\n",
    "model = SimpleCNN(input_channels, num_classes)\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(type(train_mfcc), type(train_labels), torch.tensor(train_labels))\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(torch.tensor(np.array(train_mfcc)).float().permute(0, 3, 1, 2), torch.tensor(train_labels), test_size=0.2, random_state=42)\n",
    "print(X_train.shape[0])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(10):  # 进行10个训练周期\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        # print(i)\n",
    "        # 前向传播\n",
    "        try:\n",
    "            x = X_train[i:i+batch_size]\n",
    "            y = y_train[i:i+batch_size]\n",
    "            # print(x.shape, y.shape)\n",
    "        except:\n",
    "            continue\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y.long())  # 假设的目标\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i//batch_size+1)%20 == 0:\n",
    "            print(f'Epoch [{epoch+1}/10], Batch [{i//batch_size+1}/{X_train.shape[0]//batch_size}], Loss: {loss.item():.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3234 3234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weijing\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(len(train_mfcc),len(train_labels))\n",
    "num_classes = len(set(train_labels))\n",
    "\n",
    "# 创建一个顺序模型\n",
    "model = Sequential()\n",
    "\n",
    "# 添加卷积层和池化层\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(num_mfcc, max_length, input_channels)))\n",
    "model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# 添加扁平层\n",
    "model.add(Flatten())\n",
    "\n",
    "# 添加全连接层\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))  # 对于多分类问题，最后一层使用softmax激活函数，并且输出单元数量等于类别数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2587, 20, 100, 5)\n",
      "Epoch 1/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 144ms/step - accuracy: 0.0040 - loss: 6.1359 - val_accuracy: 0.0000e+00 - val_loss: 6.3331\n",
      "Epoch 2/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 138ms/step - accuracy: 0.0050 - loss: 6.1379 - val_accuracy: 0.0000e+00 - val_loss: 6.3323\n",
      "Epoch 3/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 175ms/step - accuracy: 0.0027 - loss: 6.1395 - val_accuracy: 0.0000e+00 - val_loss: 6.3315\n",
      "Epoch 4/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 142ms/step - accuracy: 0.0015 - loss: 6.1350 - val_accuracy: 0.0000e+00 - val_loss: 6.3307\n",
      "Epoch 5/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 197ms/step - accuracy: 0.0053 - loss: 6.1293 - val_accuracy: 0.0000e+00 - val_loss: 6.3302\n",
      "Epoch 6/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 172ms/step - accuracy: 0.0034 - loss: 6.1390 - val_accuracy: 0.0000e+00 - val_loss: 6.3295\n",
      "Epoch 7/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 142ms/step - accuracy: 0.0034 - loss: 6.1329 - val_accuracy: 0.0000e+00 - val_loss: 6.3289\n",
      "Epoch 8/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 208ms/step - accuracy: 0.0031 - loss: 6.1203 - val_accuracy: 0.0000e+00 - val_loss: 6.3282\n",
      "Epoch 9/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 203ms/step - accuracy: 0.0020 - loss: 6.1395 - val_accuracy: 0.0000e+00 - val_loss: 6.3275\n",
      "Epoch 10/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 172ms/step - accuracy: 0.0030 - loss: 6.1351 - val_accuracy: 0.0000e+00 - val_loss: 6.3270\n",
      "Epoch 11/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 168ms/step - accuracy: 0.0029 - loss: 6.1342 - val_accuracy: 0.0000e+00 - val_loss: 6.3264\n",
      "Epoch 12/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 205ms/step - accuracy: 0.0018 - loss: 6.1261 - val_accuracy: 0.0000e+00 - val_loss: 6.3259\n",
      "Epoch 13/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 188ms/step - accuracy: 0.0013 - loss: 6.1288 - val_accuracy: 0.0000e+00 - val_loss: 6.3252\n",
      "Epoch 14/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 158ms/step - accuracy: 0.0031 - loss: 6.1327 - val_accuracy: 0.0000e+00 - val_loss: 6.3249\n",
      "Epoch 15/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 142ms/step - accuracy: 0.0022 - loss: 6.1267 - val_accuracy: 0.0000e+00 - val_loss: 6.3244\n",
      "Epoch 16/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 176ms/step - accuracy: 0.0042 - loss: 6.1337 - val_accuracy: 0.0000e+00 - val_loss: 6.3238\n",
      "Epoch 17/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 187ms/step - accuracy: 0.0044 - loss: 6.1253 - val_accuracy: 0.0000e+00 - val_loss: 6.3236\n",
      "Epoch 18/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 154ms/step - accuracy: 0.0058 - loss: 6.1250 - val_accuracy: 0.0000e+00 - val_loss: 6.3230\n",
      "Epoch 19/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 188ms/step - accuracy: 0.0030 - loss: 6.1251 - val_accuracy: 0.0000e+00 - val_loss: 6.3226\n",
      "Epoch 20/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 220ms/step - accuracy: 0.0032 - loss: 6.1309 - val_accuracy: 0.0000e+00 - val_loss: 6.3223\n",
      "Epoch 21/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 210ms/step - accuracy: 0.0021 - loss: 6.1212 - val_accuracy: 0.0000e+00 - val_loss: 6.3221\n",
      "Epoch 22/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 179ms/step - accuracy: 7.3028e-04 - loss: 6.1310 - val_accuracy: 0.0000e+00 - val_loss: 6.3215\n",
      "Epoch 23/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 193ms/step - accuracy: 9.5999e-04 - loss: 6.1251 - val_accuracy: 0.0000e+00 - val_loss: 6.3212\n",
      "Epoch 24/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 216ms/step - accuracy: 0.0019 - loss: 6.1216 - val_accuracy: 0.0000e+00 - val_loss: 6.3208\n",
      "Epoch 25/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 145ms/step - accuracy: 0.0043 - loss: 6.1264 - val_accuracy: 0.0000e+00 - val_loss: 6.3206\n",
      "Epoch 26/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 190ms/step - accuracy: 3.7200e-04 - loss: 6.1285 - val_accuracy: 0.0000e+00 - val_loss: 6.3204\n",
      "Epoch 27/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 243ms/step - accuracy: 0.0038 - loss: 6.1202 - val_accuracy: 0.0000e+00 - val_loss: 6.3200\n",
      "Epoch 28/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.0020 - loss: 6.1293 - val_accuracy: 0.0000e+00 - val_loss: 6.3197\n",
      "Epoch 29/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 176ms/step - accuracy: 0.0016 - loss: 6.1250 - val_accuracy: 0.0000e+00 - val_loss: 6.3194\n",
      "Epoch 30/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 206ms/step - accuracy: 0.0020 - loss: 6.1214 - val_accuracy: 0.0000e+00 - val_loss: 6.3192\n",
      "Epoch 31/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 209ms/step - accuracy: 0.0048 - loss: 6.1230 - val_accuracy: 0.0000e+00 - val_loss: 6.3189\n",
      "Epoch 32/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 163ms/step - accuracy: 0.0032 - loss: 6.1175 - val_accuracy: 0.0000e+00 - val_loss: 6.3188\n",
      "Epoch 33/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 214ms/step - accuracy: 0.0027 - loss: 6.1231 - val_accuracy: 0.0000e+00 - val_loss: 6.3185\n",
      "Epoch 34/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 206ms/step - accuracy: 7.4880e-04 - loss: 6.1171 - val_accuracy: 0.0000e+00 - val_loss: 6.3183\n",
      "Epoch 35/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.0018 - loss: 6.1219 - val_accuracy: 0.0000e+00 - val_loss: 6.3180\n",
      "Epoch 36/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 204ms/step - accuracy: 0.0016 - loss: 6.1235 - val_accuracy: 0.0000e+00 - val_loss: 6.3178\n",
      "Epoch 37/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 208ms/step - accuracy: 0.0037 - loss: 6.1192 - val_accuracy: 0.0000e+00 - val_loss: 6.3177\n",
      "Epoch 38/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 176ms/step - accuracy: 0.0031 - loss: 6.1221 - val_accuracy: 0.0000e+00 - val_loss: 6.3174\n",
      "Epoch 39/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.0032 - loss: 6.1188 - val_accuracy: 0.0000e+00 - val_loss: 6.3172\n",
      "Epoch 40/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 214ms/step - accuracy: 3.7935e-04 - loss: 6.1201 - val_accuracy: 0.0000e+00 - val_loss: 6.3171\n",
      "Epoch 41/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 236ms/step - accuracy: 0.0026 - loss: 6.1221 - val_accuracy: 0.0000e+00 - val_loss: 6.3171\n",
      "Epoch 42/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.0026 - loss: 6.1173 - val_accuracy: 0.0000e+00 - val_loss: 6.3168\n",
      "Epoch 43/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 214ms/step - accuracy: 0.0018 - loss: 6.1213 - val_accuracy: 0.0000e+00 - val_loss: 6.3166\n",
      "Epoch 44/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 256ms/step - accuracy: 0.0039 - loss: 6.1168 - val_accuracy: 0.0000e+00 - val_loss: 6.3165\n",
      "Epoch 45/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 190ms/step - accuracy: 0.0018 - loss: 6.1243 - val_accuracy: 0.0000e+00 - val_loss: 6.3164\n",
      "Epoch 46/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 189ms/step - accuracy: 9.7084e-04 - loss: 6.1209 - val_accuracy: 0.0000e+00 - val_loss: 6.3162\n",
      "Epoch 47/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 208ms/step - accuracy: 0.0012 - loss: 6.1212 - val_accuracy: 0.0000e+00 - val_loss: 6.3161\n",
      "Epoch 48/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 168ms/step - accuracy: 0.0019 - loss: 6.1180 - val_accuracy: 0.0000e+00 - val_loss: 6.3160\n",
      "Epoch 49/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 188ms/step - accuracy: 0.0014 - loss: 6.1149 - val_accuracy: 0.0000e+00 - val_loss: 6.3158\n",
      "Epoch 50/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 205ms/step - accuracy: 0.0027 - loss: 6.1163 - val_accuracy: 0.0000e+00 - val_loss: 6.3157\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.0020 - loss: 6.1854\n",
      "Test accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "# 编译模型\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(train_mfcc), to_categorical(np.array(train_labels), num_classes), test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "# 训练模型\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "# 评估模型\n",
    "loss, accuracy = model.evaluate(np.array(test_mfcc), to_categorical(np.array(test_labels), num_classes))\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")\n",
    "# 使用模型进行预测\n",
    "# predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 评价指标(准确率Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 请编写代码或使用库函数accuracy_score计算测试集上的准确率Accuracy\n",
    "\n",
    "## 5. 评价指标(准确率Accuracy)\n",
    "\n",
    "# 定义函数计算预测结果\n",
    "def predict(models, features):\n",
    "    predictions = []\n",
    "    for feature in features:\n",
    "        max_score = float(\"-inf\")\n",
    "        predicted_label = None\n",
    "        for label, model in models.items():\n",
    "            score = model.score(feature)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                predicted_label = label\n",
    "        predictions.append(predicted_label)\n",
    "    return predictions\n",
    "\n",
    "# 进行预测\n",
    "test_predictions = predict(trained_models, test_mfcc)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"测试集准确率:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6. 分析和可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 请使用matplotlib等可视化库对你的实验结果进行可视化分析。\n",
    "## 包括但不限于准确率的对比、错误分类的分析、特征的影响等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 结果讨论\n",
    "讨论你的模型性能，尝试解释为什么某些模型比其他模型表现好，以及可能的改进方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 保存模型（可选）\n",
    "如果需要，可以在这里添加代码保存你的模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
